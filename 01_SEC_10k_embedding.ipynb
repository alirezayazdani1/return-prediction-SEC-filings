{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7906e74-b766-4c85-9024-a2856cff50c5",
   "metadata": {},
   "source": [
    "# Generating Document Vectors for SEC Filings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72378a-bde5-4aa2-8014-d7ff40f283f9",
   "metadata": {},
   "source": [
    "Many tasks require embeddings of domain-specific vocabulary that models pretrained on a generic corpus may not be able to capture. Standard **Word2Vec** models are not able to\r",
    "assign vectors to out-of-vocabulary words and instead use a default vector that reduces their predictive value. For example, when working with industry-specific documents, the vocabulary or its usage may change over time as new technologies or products emerge. As a result, the embeddings need to evolve as well. In addition, documents like corporate earnings releases use nuanced language that GloVe vectors pretrained on Wikipedia articles are unlikely to properly reflect.\n",
    "\n",
    "**Doc2Vec** is a model that represents each document as a vector, which usually outperforms simple-averaging of Word2Vec vectors. Given the specific content of each section of SEC filings, training document embeddings instead of single word embeddings can increase the predictive content of each section in the downstream tasks.\n",
    "\n",
    "The document embedding model, produces embeddings for pieces of text like a paragraph or a product review directly. Similar to Word2Vec, there are also two flavors of Doc2Vec:\n",
    "- **The distributed memory (DM)** model corresponds to the Word2Vec CBOW model. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document's doc-vector.\n",
    "- **The distributed bag of words (DBOW)** model corresponds to the Word2Vec skip-gram architecture. The doc vectors result from training a neural net to predict a target word using the full document's doc vector.\n",
    "\n",
    "Gensim's [Doc2Vec class](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html) implements above algorithms. We use [Gensim](https://radimrehurek.com/gensim/intro.html) library for training embeddings that represent documents as vectors. Gensim is a free open-source Python library for representing documents as semantic vectors. Gensim is designed to process raw, unstructured texts using unsupervised machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23197d79-630c-42e6-ae7b-db937aec1a91",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafd107d-a35e-48db-bb33-dd26b2a07a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5add89-d3ff-4141-b1a9-e27b06260ae0",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "We will use [Hugging Face dataset](https://huggingface.co/datasets/c3po-ai/edgar-corpus) that contains **10-K** annual reports of public companies from 1993-2020 from SEC EDGAR filings. We extract the most informative sections, namely:\n",
    "- Sections 1 and 1A: Business and Risk Factors\n",
    "- Sections 7 and 7A: Management's Discussion and Disclosures about Market Risks\n",
    "\n",
    "For about 3,000 companies, we have stock prices between 2013-2016 to label the data for predictive modeling. Therefore, we use the years 2010-2015 for training document embeddings and 2016's data for testing. In addition, for downstream tasks such as predicting company's returns, we select a subset of the companies (large, medium and small caps) to reduce computation times.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a826fa-d71d-439e-931e-fbb161b61430",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROC = 4\n",
    "\n",
    "SECTIONS = [\"cik\", \"year\", \"section_1\", \"section_1A\", \"section_7\", \"section_7A\"]\n",
    "\n",
    "FROM_DISK = True\n",
    "\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2016\n",
    "\n",
    "# The sentence transformer language model: all-MiniLM-L6-v2 has embedding size of 384\n",
    "# Set the same for Gensim embedding size for consistent comparisons\n",
    "EMBEDDING_SIZE = 384\n",
    "\n",
    "results_path = Path('sec-edgar-10k')\n",
    "\n",
    "model_path = results_path / 'models'\n",
    "data_path = results_path / 'data'\n",
    "parsed_data_path = results_path / 'parsed-data'\n",
    "companies_data_path = results_path / 'subset-companies'\n",
    "log_path = results_path / 'logs'\n",
    "\n",
    "for path in [model_path, data_path, parsed_data_path, companies_data_path, log_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_path / 'doc2vec.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f34cd2-d10a-4b65-bbdc-ca22a7524145",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FROM_DISK:\n",
    "    years = {}\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        years[year] = datasets.load_dataset(\"eloukas/edgar-corpus\", f\"year_{year}\", split=\"all\")\n",
    "    \n",
    "    train = datasets.concatenate_datasets([years[year] for year in range(START_YEAR, END_YEAR)], axis=0)\n",
    "    test = years[END_YEAR]\n",
    "    train = train.remove_columns(list(set(train.column_names) - set(SECTIONS)))\n",
    "    test = test.remove_columns(list(set(test.column_names) - set(SECTIONS)))\n",
    "    \n",
    "    dataset = train.train_test_split(train_size=0.8)\n",
    "    dataset[\"validation\"] = dataset.pop(\"test\")\n",
    "    dataset[\"test\"] = test\n",
    "    \n",
    "    dataset.save_to_disk(data_path.as_posix(), num_proc=NUM_PROC)\n",
    "else:\n",
    "    dataset = datasets.load_from_disk(data_path.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9d672-ca5c-4a17-82e8-672dec93da3f",
   "metadata": {},
   "source": [
    "## Document Embeddings: Gensim\n",
    "\n",
    "We use **Spacy** to preprocess and filter the paragraphs e.g., removing stop words, digits, punctuations etc. The model is trained on all 4 sections of SEC filings corpus using distributed memory, which is analogous to Word2Vec skip-gram. The doc vectors are obtained by training a neural network using Doc2Vec's **distributed memory** algorithm of Gensim library.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3567db-4e40-48d2-bb61-26c10b3b1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from gensim.models import KeyedVectors, Doc2Vec, doc2vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46321010-bda5-40e6-aeaa-422ced7ced9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.max_length = 10_000_000\n",
    "\n",
    "SECTIONS_TO_PARSE = ['section_1', 'section_1A', 'section_7', 'section_7A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a244d924-ce3f-4fac-bd4a-4b993fb264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]], axis=0)\n",
    "test = dataset[\"test\"]\n",
    "if int(sorted(set(train['year']))[-1]) >= END_YEAR:\n",
    "    test = train.filter(lambda row: int(row['year']) == END_YEAR)\n",
    "    train = train.filter(lambda row: int(row['year']) >= START_YEAR and int(row['year']) < END_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531cfdc2-0a64-4c74-b59d-c2fc7a0e51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sections(examples, col_names):\n",
    "    sections = {}\n",
    "\n",
    "    for col in col_names:\n",
    "        docs = []\n",
    "        for text in examples[col]:\n",
    "            doc = nlp(text)\n",
    "            clean_sentence = []\n",
    "            for sentence in doc.sents:\n",
    "                if sentence is not None:\n",
    "                    for token in sentence:\n",
    "                        if not any([token.is_stop,\n",
    "                                    token.is_digit,\n",
    "                                    not token.is_alpha,\n",
    "                                    token.is_punct,\n",
    "                                    token.is_space,\n",
    "                                    token.lemma_ == '-PRON-',\n",
    "                                    token.pos_ in ['PUNCT', 'SYM', 'X']]):\n",
    "                            clean_sentence.append(token.text)\n",
    "            if len(clean_sentence) > 0:\n",
    "                docs.append(' '.join(clean_sentence))\n",
    "            else:\n",
    "                docs.append(\"\")\n",
    "        sections[col] = docs\n",
    "\n",
    "    res = {\"cik\": examples[\"cik\"], \"year\": examples[\"year\"]}\n",
    "    for section in sections:\n",
    "        res[f\"parsed_{section}\"] = sections[section]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82be814-fb80-4b6e-b669-4eb54ce99951",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FROM_DISK:\n",
    "    train_parsed = train.map(parse_sections, batched=True, fn_kwargs={\"col_names\": SECTIONS_TO_PARSE}, remove_columns=train.column_names, num_proc=NUM_PROC)\n",
    "    test_parsed = test.map(parse_sections, batched=True, fn_kwargs={\"col_names\": SECTIONS_TO_PARSE}, remove_columns=test.column_names, num_proc=NUM_PROC)\n",
    "    \n",
    "    dataset = datasets.DatasetDict()\n",
    "    dataset['train'] = train_parsed\n",
    "    dataset['test'] = test_parsed\n",
    "    \n",
    "    dataset.save_to_disk(parsed_data_path.as_posix(), num_proc=NUM_PROC)\n",
    "else:\n",
    "    dataset = datasets.load_from_disk(parsed_data_path.as_posix())\n",
    "    train_parsed = dataset['train']\n",
    "    test_parsed = dataset['test']\n",
    "    if int(sorted(set(train_parsed['year']))[-1]) >= END_YEAR:\n",
    "        test_parsed = train_parsed.filter(lambda row: int(row['year']) == END_YEAR)\n",
    "        train_parsed = train_parsed.filter(lambda row: int(row['year']) >= START_YEAR and int(row['year']) < END_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf602791-479e-4647-9850-91113e82d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSED_SECTIONS = set(train_parsed.column_names) - set(['cik', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65bff678-ad98-49a7-a787-4f50bbd143a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(data, col_names, new_name):\n",
    "    wrapped = []\n",
    "    for col in col_names:\n",
    "        temp = data.select_columns(['cik', 'year', col])\n",
    "        temp = temp.filter(lambda example: example[col] != '')\n",
    "        temp = temp.add_column(name='section',  column=[col.split('_')[-1]] * len(temp))\n",
    "        temp = temp.rename_column(col, new_name)\n",
    "        wrapped.append(temp)\n",
    "\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c24d803-98e1-4002-8130-a671c073fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all parsed sections of SEC filings in one column to be fed to the gensim model for training\n",
    "wrapped = wrap(train_parsed, PARSED_SECTIONS, new_name='parsed_sections')\n",
    "train_parsed = datasets.concatenate_datasets(wrapped, axis=0)\n",
    "\n",
    "wrapped = wrap(test_parsed, PARSED_SECTIONS, new_name='parsed_sections')\n",
    "test_parsed = datasets.concatenate_datasets(wrapped, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7945796a-820a-4b78-88d0-2d01540b7e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 182218\n",
      "Number of test examples: 27178\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {train_parsed.num_rows}\")\n",
    "print(f\"Number of test examples: {test_parsed.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8fe26d8-1ce8-4582-8ad2-f5e1e2b0eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, data, col_name):\n",
    "        self.data = data\n",
    "        self.data.set_format('pandas')\n",
    "        self.df = self.data[col_name]\n",
    "    \n",
    "    \"\"\"An iterator that yields sentences (lists of str)\"\"\"\n",
    "    def __iter__(self):\n",
    "        for i, line in enumerate(self.df):\n",
    "            # lowercase and tokenize\n",
    "            tokens = simple_preprocess(line)\n",
    "            # for training data, add tags\n",
    "            yield doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2cd281-220e-4126-adf2-cef9bde4a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = Corpus(train_parsed, col_name='parsed_sections')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00518115-cb9f-4f7f-9fb8-80628bc17f07",
   "metadata": {},
   "source": [
    "### Train and save the model or load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae1fcae-5f0b-442a-ad3f-710869fade58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists((model_path / 'doc2vec_0.model').as_posix()):\n",
    "    gensim_model = Doc2Vec.load((model_path / 'doc2vec_0.model').as_posix())\n",
    "    wv = KeyedVectors.load((model_path / 'word_vectors_0.bin').as_posix())\n",
    "else:\n",
    "    gensim_model = Doc2Vec(\n",
    "        documents=train_corpus,\n",
    "        dm=1,              # 1=dist. memory, 0=dist. BOW\n",
    "        vector_size=EMBEDDING_SIZE,\n",
    "        window=8,          # max distance between target and context\n",
    "        min_count=2,       # ignore tokens w. lower frequency\n",
    "        epochs=15,\n",
    "        alpha=0.05,        # initial learning rate\n",
    "        min_alpha=0.001,   # final learning rate\n",
    "        hs=0,              # 1=hierarchical softmax, 0=negative sampling\n",
    "        negative=14,       # negative training (noise) samples, only needed for negative sampling\n",
    "        dm_concat=0,       # 1=concatenate vectors, 0=sum\n",
    "        dbow_words=0,      # 1=train word vectors as well (in skip-gram fashion), 0=only doc. vectors\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    gensim_model.save((model_path / 'doc2vec_0.model').as_posix())\n",
    "    gensim_model.wv.save((model_path / 'word_vectors_0.bin').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe2a5b-ec45-4d70-aa5b-2c21a10f5e66",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae2849c2-ee52-4cb5-8a42-3d0c73a0bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(test, train, model, topn=10):\n",
    "    top = {}\n",
    "    sim_docs = {}\n",
    "    \n",
    "    for doc_id in range(len(test)):\n",
    "        inferred_vector = model.infer_vector(list(test)[doc_id].words)\n",
    "        sims = model.dv.most_similar(positive=[inferred_vector], topn=topn)\n",
    "        docids = [docid for docid, sim in sims]\n",
    "        cik = test.data[:]['cik'].iloc[doc_id]\n",
    "        year = test.data[:]['year'].iloc[doc_id]\n",
    "        section = test.data[:]['section'].iloc[doc_id]\n",
    "        top_n = train.data[:][['cik', 'year', 'section']].loc[docids].values\n",
    "        top[(cik, year, section)] = (*list(top_n[0]), sims[0][1])\n",
    "        sim_docs[cik] = dict(Counter(top_n[:, 0]))\n",
    "\n",
    "    accuracy = 0\n",
    "    for k in top:\n",
    "        c, y, s = k\n",
    "        if top[k][0] == c and top[k][1] == y and top[k][2] == s:\n",
    "            accuracy += 1\n",
    "    \n",
    "    print(f\"Model accuracy in selecting the exact same document: {accuracy / len(top) * 100}%\")\n",
    "\n",
    "    accuracy = 0\n",
    "    for k in top:\n",
    "        c, y, s = k\n",
    "        if top[k][0] == c:\n",
    "            accuracy += 1\n",
    "\n",
    "    print(f\"Model accuracy in selecting documents from the same company: {accuracy / len(top) * 100}%\")\n",
    "\n",
    "    return top, sim_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb0556-712f-4519-98d0-38fa1c2d9bfa",
   "metadata": {},
   "source": [
    "#### Sanity check using a small validation set of documents from train corpus\n",
    "\n",
    "To assess the model, we will first infer new vectors for random sample documents of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. Basically, we are pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that we have likely overfit our model and so we should be able to find similar documents very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "231b0878-a60c-4fea-876c-ba7a59a930ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = np.random.randint(0, len(train_parsed), size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "508a27ed-1a4d-41aa-9964-f9f0d8840e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_corpus = Corpus(train_parsed.select(doc_ids), col_name='parsed_sections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92531ad4-e04d-4416-b00e-97ffdb9a5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy in selecting the exact same document: 66.0%\n",
      "Model accuracy in selecting documents from the same company: 84.0%\n"
     ]
    }
   ],
   "source": [
    "top, sim_docs = eval_model(valid_corpus, train_corpus, gensim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685baa6e-6118-4fbb-beed-62cb2c7c529c",
   "metadata": {},
   "source": [
    "### Continue training if needed\n",
    "\n",
    "We may need to continue training to increase the validation set accuracy in selecting the same document used for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9e8d7b-7dd7-4c2a-be36-cd76d0d791ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists((model_path / 'doc2vec_1.model').as_posix()):\n",
    "    gensim_model = Doc2Vec.load((model_path / 'doc2vec_1.model').as_posix())\n",
    "    wv = KeyedVectors.load((model_path / 'word_vectors_1.bin').as_posix())\n",
    "else:\n",
    "    gensim_model.train(train_corpus, total_examples=gensim_model.corpus_count, epochs=gensim_model.epochs)\n",
    "    \n",
    "    gensim_model.save((model_path / 'doc2vec_1.model').as_posix())\n",
    "    gensim_model.wv.save((model_path / 'word_vectors_1.bin').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec3ef89c-5dfa-4f3c-917c-99c3aa86b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy in selecting the exact same document: 68.0%\n",
      "Model accuracy in selecting documents from the same company: 84.0%\n"
     ]
    }
   ],
   "source": [
    "top, sim_docs = eval_model(valid_corpus, train_corpus, gensim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955d8e3-53d1-48c2-8243-4ce308f96461",
   "metadata": {},
   "source": [
    "## Document Embeddings Using Sentence Transformers\n",
    "\n",
    "Doc2Vec embeddings allow only a single fixed-length representation of each token that does not differentiate between context-specific usages. To address problems such as multiple meanings for the same word, called *polysemy*, several new models have emerged that build on the **attention** mechanism designed to learn more contextualized word embeddings. The key characteristics of these models are as follows:\r",
    "- The use of bidirectional language models that process text both left-to-right and right-to-left for a richer context representation\r",
    "- The use of semi-supervised pretraining on a large generic corpus to learn universal language aspects in the form of embeddings and network weights that can be used and fine-tuned for specific tasks.\n",
    "\n",
    "To this end, we use **SentenceTransformers**, which is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described the paper [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084). It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "The fine-tuned model is adopted from the [HuggingFace model repository](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). In short, the pretrained `nreimers/MiniLM-L6-H384-uncased` model is fine-tuned on a 1B sentence pairs dataset. A self-supervised contrastive learning objective has been used to fine-tune the **BERT encoder**: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in the dataset.\n",
    "\n",
    "### Intended Use\n",
    "It is important to note that this model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks. The output embeddings are tuned for similarity and clustering tasks, hence not fully tailored to our task of predicting returns given SEC filings.\n",
    "\n",
    "Nevertheless, as a pedagogical example, it will be interesting to show how a model like this can be adopted in our use case even though it is not specifically fine-tuned using financial text data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad688fa-b223-4cac-abb5-3c58a5094987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b4e4bc5-8a6f-48a2-a10c-6e27fba8df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "# this model is fine tuned on nreimers/MiniLM-L6-H384-uncased pre-trained model\n",
    "model_ckpt = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
    "sbert_model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)\n",
    "\n",
    "MAX_LENGTH = tokenizer.max_len_single_sentence\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14aed0f2-2f42-43f1-beeb-f1bc357e630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask, sample_map):\n",
    "    batch_result = []\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = tf.expand_dims(tf.cast(attention_mask, tf.float32), -1)\n",
    "    # Perform mean pooling over chunks of each sequence\n",
    "    for i in range(BATCH_SIZE):\n",
    "        s_idx = np.where(sample_map==i)[0][0]\n",
    "        e_idx = np.where(sample_map==i)[0][-1] + 1\n",
    "        sum_embeddings = tf.reduce_sum(token_embeddings[s_idx:e_idx] * input_mask_expanded[s_idx:e_idx], axis=[0, 1])\n",
    "        sum_mask = tf.clip_by_value(tf.reduce_sum(input_mask_expanded[s_idx:e_idx]), clip_value_min=1e-7, clip_value_max=np.inf)\n",
    "        batch_result += [sum_embeddings / sum_mask]\n",
    "        \n",
    "    return tf.stack(batch_result)\n",
    "\n",
    "def get_embeddings(data, col_name):\n",
    "    encoded_input = tokenizer(\n",
    "        data[col_name],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_overflowing_tokens=True,  # Large sequences are chunked into arrays of max_length\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    # Identifies what sequence chunk belongs to which sample\n",
    "    sample_map = encoded_input.pop(\"overflow_to_sample_mapping\")\n",
    "    attention_mask = encoded_input[\"attention_mask\"]\n",
    "    model_output = sbert_model(**encoded_input)\n",
    "    \n",
    "    return {\n",
    "        \"cik\": data[\"cik\"],\n",
    "        \"year\": data[\"year\"],\n",
    "        \"section\": data[\"section\"],\n",
    "        f\"{col_name}_embedding\": mean_pooling(model_output, attention_mask, sample_map)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cac9253-37f5-48f3-953c-95d2add211d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all sections of raw SEC filings in one column to be fed to the transformer model\n",
    "wrapped = wrap(train, SECTIONS_TO_PARSE, new_name='raw_sections')\n",
    "train = datasets.concatenate_datasets(wrapped, axis=0)\n",
    "\n",
    "wrapped = wrap(test, SECTIONS_TO_PARSE, new_name='raw_sections')\n",
    "test = datasets.concatenate_datasets(wrapped, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "162eebbf-095e-49ce-993c-d4e579637ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings for SEC filings data using a sentence transformer for ['2010', '2011', '2012', '2013', '2014', '2015'] as train and {'2016'} as test data.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate embeddings for SEC filings data using a sentence transformer for {sorted(set(train['year']))} as train and {set(test['year'])} as test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29984c-3498-470d-873f-e670ee8e5817",
   "metadata": {},
   "source": [
    "Embedding calculations using sentence transformer on a local machine would take a long time for all SEC dataset. We postpone the performance analysis of sentence transformer to the 3rd notebook where we select a subset of 750 companies for train/test a booster model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
