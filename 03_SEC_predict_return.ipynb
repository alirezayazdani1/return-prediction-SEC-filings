{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8946e940-bd8d-410e-bbef-d9d5d660a3b9",
   "metadata": {},
   "source": [
    "# Return Predictions Using SEC Filings Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdf7fc-79a1-430a-a553-76408472978e",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d776a8-caab-4503-96fe-225fa99796c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datasets\n",
    "\n",
    "from gensim.models import KeyedVectors, Doc2Vec, doc2vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71a27c48-e1b4-4437-bb94-af8584c4cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROC = 4\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "sec_path = Path('sec-edgar-10k')\n",
    "\n",
    "# results_path = Path('results', 'sec-filings')\n",
    "model_path = sec_path / 'models'\n",
    "log_path = sec_path / 'logs'\n",
    "data_path = sec_path / 'data'\n",
    "parsed_data_path = sec_path / 'parsed-data'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_path / 'lgbm.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa3219-b6ed-444a-a7b6-57b23f0a8b30",
   "metadata": {},
   "source": [
    "## Prepare Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87703f-f26d-4455-8622-ab585d77ca8f",
   "metadata": {},
   "source": [
    "### Get Stock Prices and SEC Filing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1fb4e5-f872-4815-8e0f-e960649cea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_index = pd.read_csv(sec_path / 'filing_index.csv', parse_dates=['DATE_FILED']).rename(columns=str.lower)\n",
    "filing_index.index += 1   # add 1 to match filing index with filings in price data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a87f26-1377-4abc-986c-d69bf0f5f9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>company_name</th>\n",
       "      <th>form_type</th>\n",
       "      <th>date_filed</th>\n",
       "      <th>edgar_link</th>\n",
       "      <th>quarter</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sic</th>\n",
       "      <th>exchange</th>\n",
       "      <th>hits</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000180</td>\n",
       "      <td>SANDISK CORP</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2013-02-19</td>\n",
       "      <td>edgar/data/1000180/0001000180-13-000009.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>SNDK</td>\n",
       "      <td>3572</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000209</td>\n",
       "      <td>MEDALLION FINANCIAL CORP</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2013-03-13</td>\n",
       "      <td>edgar/data/1000209/0001193125-13-103504.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>TAXI</td>\n",
       "      <td>6199</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000228</td>\n",
       "      <td>HENRY SCHEIN INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>edgar/data/1000228/0001000228-13-000010.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>HSIC</td>\n",
       "      <td>5047</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000229</td>\n",
       "      <td>CORE LABORATORIES N V</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2013-02-19</td>\n",
       "      <td>edgar/data/1000229/0001000229-13-000009.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>CLB</td>\n",
       "      <td>1389</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000232</td>\n",
       "      <td>KENTUCKY BANCSHARES INC  KY</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>edgar/data/1000232/0001104659-13-025094.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>KTYB</td>\n",
       "      <td>6022</td>\n",
       "      <td>OTC</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cik                  company_name form_type date_filed  \\\n",
       "1  1000180                  SANDISK CORP      10-K 2013-02-19   \n",
       "2  1000209      MEDALLION FINANCIAL CORP      10-K 2013-03-13   \n",
       "3  1000228              HENRY SCHEIN INC      10-K 2013-02-13   \n",
       "4  1000229         CORE LABORATORIES N V      10-K 2013-02-19   \n",
       "5  1000232  KENTUCKY BANCSHARES INC  KY       10-K 2013-03-28   \n",
       "\n",
       "                                    edgar_link  quarter ticker   sic exchange  \\\n",
       "1  edgar/data/1000180/0001000180-13-000009.txt        1   SNDK  3572   NASDAQ   \n",
       "2  edgar/data/1000209/0001193125-13-103504.txt        1   TAXI  6199   NASDAQ   \n",
       "3  edgar/data/1000228/0001000228-13-000010.txt        1   HSIC  5047   NASDAQ   \n",
       "4  edgar/data/1000229/0001000229-13-000009.txt        1    CLB  1389     NYSE   \n",
       "5  edgar/data/1000232/0001104659-13-025094.txt        1   KTYB  6022      OTC   \n",
       "\n",
       "  hits  year  \n",
       "1    3  2013  \n",
       "2    0  2013  \n",
       "3    3  2013  \n",
       "4    2  2013  \n",
       "5    0  2013  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filing_index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba63ec-01ba-495b-b88f-3de9a02dcbda",
   "metadata": {},
   "source": [
    "### Create weekly forward returns\n",
    "\n",
    "We will compute (somewhat arbitrarily) five-day forward returns for the day of filing (or the day before if there are no prices for that date), assuming that filing occurred after market hours. Clearly, this assumption could be wrong, underscoring the need for **point-in-time data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5017601e-cd76-4436-8cf8-e7891145faf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:32:43.198031Z",
     "start_time": "2021-02-23T18:32:43.009584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1228824 entries, 2013-09-17 00:00:00-04:00 to 2015-01-23 00:00:00\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   filing  1228824 non-null  int64  \n",
      " 1   ticker  1228824 non-null  object \n",
      " 2   cik     1228824 non-null  int64  \n",
      " 3   open    1228802 non-null  float64\n",
      " 4   high    1228820 non-null  float64\n",
      " 5   low     1228820 non-null  float64\n",
      " 6   close   1228821 non-null  float64\n",
      " 7   volume  1228824 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 84.4+ MB\n"
     ]
    }
   ],
   "source": [
    "prices = pd.read_hdf(sec_path / 'sec_returns.h5', 'prices')\n",
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b68142-f4a0-4521-a023-bffd91cfc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filings = prices.filing.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb02f2-2421-4471-a519-901b3b7c4f1c",
   "metadata": {},
   "source": [
    "We compute the forward returns as follows, removing outliers with weekly returns below\n",
    "50 or above 100 percent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1714c31f-9cfa-4ce6-ab6c-976652734c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:33:19.708318Z",
     "start_time": "2021-02-23T18:32:43.199003Z"
    }
   },
   "outputs": [],
   "source": [
    "years = set()\n",
    "fwd_return = {}\n",
    "\n",
    "for filing in filings:\n",
    "    date_filed = filing_index.at[filing, 'date_filed']\n",
    "    price_data = prices[prices.filing==filing].close.sort_index()\n",
    "    date_filed = date_filed.tz_localize(price_data.index[0].tz)\n",
    "    cik = filing_index.at[filing, 'cik']\n",
    "    year = date_filed.year\n",
    "    years.add(year)\n",
    "    \n",
    "    try:\n",
    "        r = (price_data\n",
    "             .pct_change(periods=5)\n",
    "             .shift(-5)\n",
    "             .loc[:date_filed]\n",
    "             .iloc[-1])\n",
    "    except:\n",
    "        continue\n",
    "    if not np.isnan(r) and -.5 < r < 1:\n",
    "        fwd_return[(str(cik), str(year))] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c8c721-be76-4032-bbd4-9810a79b9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = pd.MultiIndex.from_tuples(list(fwd_return.keys()), names=['cik', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c79476e-f1e5-4ff6-9d48-e5cab4734452",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.DataFrame([], index=multi_index, columns=['returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48652c2-a692-4eaa-91d7-a437fc0d88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in fwd_return.items():\n",
    "    returns.loc[k, 'returns'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2625236-cf14-4f49-ad51-feed1b38ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIK_YEAR = returns.index.to_list()\n",
    "years = sorted(list(years))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe3cfb-8c08-4bd5-8822-977ae9512e9d",
   "metadata": {},
   "source": [
    "## Train Gradient Boosting Model with SEC Document Embeddings from Trained Gensim Model\n",
    "\n",
    "We will use the embedding model (named `doc2vec_1.model`) that has been trained in the first notebook using Gensim to generate the doc-vectors of SEC filings for train and test data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c7c2d5-d336-457c-9e0f-c316d8ce290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter and process SEC filings data for [2013, 2014, 2015] as train and 2016 as test data.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filter and process SEC filings data for {years[:-1]} as train and {years[-1]} as test data.\")\n",
    "TEST_YEAR = years[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6724ae97-2ba5-41b4-a269-8749f0e04f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(parsed_data_path):\n",
    "    dataset = datasets.load_from_disk(parsed_data_path.as_posix())\n",
    "\n",
    "train_parsed = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5bda3e5-dea6-4c41-92dd-e68b64c94907",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path / 'doc2vec_1.model'):\n",
    "    gensim_model = Doc2Vec.load((model_path / 'doc2vec_1.model').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c407e8f2-2213-4780-bb6a-07368dbbda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAMES = set(train_parsed.column_names) - set(['cik', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8bbfc7-10d3-4132-819a-bc0b618fd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those companies for which we have price data from 2013-2016\n",
    "train_parsed = train_parsed.filter(lambda examples: (examples['cik'], examples['year']) in CIK_YEAR, num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f45bfa-8f5f-4314-973c-9a47c539c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_embeddings(examples, model, col_name):\n",
    "    ciks = []\n",
    "    years = []\n",
    "    items = []\n",
    "    \n",
    "    for doc, cik, year in zip(examples[col_name], examples['cik'], examples['year']):\n",
    "        tokens = simple_preprocess(doc)\n",
    "        vector = model.infer_vector(tokens)\n",
    "        ciks += [cik]\n",
    "        years += [year]\n",
    "        items += [vector]\n",
    "\n",
    "    return {'cik': ciks, 'year': years, f\"{col_name}_vec\": items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2146d81c-63a7-4ece-9fe2-d21019a5743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_to_pandas(model, data, select_size=None):\n",
    "    dfs = []\n",
    "    if select_size:\n",
    "        data = data.select(range(select_size))\n",
    "\n",
    "    for col in COL_NAMES:\n",
    "        dfs += [\n",
    "            data.map(\n",
    "                get_doc2vec_embeddings, batched=True, batch_size=BATCH_SIZE, num_proc=NUM_PROC, remove_columns=data.column_names, fn_kwargs={'model': model, 'col_name': col}\n",
    "            ).to_pandas()\n",
    "        ]\n",
    "\n",
    "    final = dfs[0][['cik' , 'year']].set_index(['cik' , 'year'], drop=True)\n",
    "    for df in dfs:\n",
    "        df.set_index(['cik' , 'year'], drop=True, inplace=True)\n",
    "        final = final.join(df, how='inner')\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c4c2db2-a677-468a-987a-84b6f0390783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced59b56c9c1475eb8dab121c55cd657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4d60ac052b4c3987e634cf0622f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8f852690374c9a844947f8ee4faa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599533bc27694d64afebc81243846119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val_test = process_to_pandas(gensim_model, train_parsed, select_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d702-db0d-46f7-9cdb-5416f53655c4",
   "metadata": {},
   "source": [
    "### Combine returns with filings doc vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77668eaa-656e-43e1-9295-adb0d9f1f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test = train_val_test.join(returns, how='inner')\n",
    "train_val_test.to_hdf(sec_path / 'sec_returns.h5', 'data/doc_vecs_return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978da31-f436-4307-b8a6-f0b14a519247",
   "metadata": {},
   "source": [
    "### Split data and train a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6761ea2f-e64c-4985-86f5-c4d0b0a4b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test = pd.read_hdf(sec_path / 'sec_returns.h5', 'data/doc_vecs_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "2c42a01b-c88f-497f-ae53-b34c0403a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(data, features, constraint_features):\n",
    "    df = data.reset_index()\n",
    "    constraints = []\n",
    "    \n",
    "    if not constraint_features:\n",
    "        # stack all SEC sections in one column and add section number as a categorical feature\n",
    "        # total number of features = embedding size + section number (384 + 1)\n",
    "        temp = pd.DataFrame([], columns=['cik', 'year', 'returns', 'features', 'section'])\n",
    "        for col in features:\n",
    "            df_sub = df[['cik', 'year', 'returns', col]].rename(columns={col: 'features'})\n",
    "            df_sub['section'] = col.split('_')[2] if 'section' not in df.columns else df['section']\n",
    "            temp = pd.concat([temp, df_sub], axis=0, ignore_index=True)\n",
    "        embedding_size = df[col].iloc[0].shape[0]\n",
    "        categories = list(temp.section.astype('category').unique())\n",
    "        temp['year'] = temp['year'].astype(np.int32)\n",
    "        temp['returns'] = temp['returns'].astype(np.float32)\n",
    "        temp['section'] = temp['section'].astype('category').cat.codes\n",
    "        temp['features'] = temp[['features', 'section']].apply(lambda x: np.append(x['features'], x['section']), axis=1)\n",
    "        train_val = temp.loc[temp.year != TEST_YEAR]\n",
    "        test = temp.loc[temp.year == TEST_YEAR]\n",
    "        cat_idx = [embedding_size]\n",
    "    else:\n",
    "        # concatanate all section embeddings horizontally and add constraint for each section within the feature\n",
    "        # vectors. Also add section number as a categorical feature\n",
    "        # total number of features = (number of sections) * (embedding size) + section number (4*384 + 1)\n",
    "        df['features'] = df[features].apply(lambda x: np.stack([x.values[i] for i in range(x.values.shape[0])]).reshape(-1, 1).squeeze(), axis=1)\n",
    "        for col in features:\n",
    "            df['section'] = col.split('_')[2]\n",
    "        embedding_size = df[col].iloc[0].shape[0]\n",
    "        categories = list(df.section.astype('category').unique())\n",
    "        df['year'] = df['year'].astype(np.int32)\n",
    "        df['returns'] = df['returns'].astype(np.float32)\n",
    "        df['section'] = df['section'].astype('category').cat.codes\n",
    "        df['features'] = df[['features', 'section']].apply(lambda x: np.append(x['features'], x['section']), axis=1)\n",
    "        train_val = df.loc[df.year != TEST_YEAR]\n",
    "        test = df.loc[df.year == TEST_YEAR]\n",
    "        constraints = [list(range(i*embedding_size, (i+1)*embedding_size)) + [embedding_size*len(features)] for i in range(len(features))]\n",
    "        cat_idx = [embedding_size*len(features)]\n",
    "\n",
    "    # split train data into 90-10 train-validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(np.stack(train_val['features'].tolist()), train_val['returns'].values, test_size=0.1)\n",
    "    X_test, y_test = np.stack(test['features'].tolist()), test['returns'].values\n",
    "\n",
    "    train_data = lgb.Dataset(data=X_train, label=y_train, categorical_feature=cat_idx, free_raw_data=False)\n",
    "    valid_data = train_data.create_valid(data=X_val, label=y_val)\n",
    "    \n",
    "    return (train_data, valid_data), (X_test, y_test), constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "ef4edf80-60ca-4e52-8be8-07cb86a56150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_booster_and_test(data, params, constraint_features=False):\n",
    "    (train, valid), test, constraints = prepare_data_for_training(data, [col + \"_vec\" for col in COL_NAMES], constraint_features)\n",
    "\n",
    "    params['interaction_constraints'] = constraints\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params=params,\n",
    "        train_set=train,\n",
    "        valid_sets=valid,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
    "    )\n",
    "    if constraint_features:\n",
    "        bst.save_model(model_path / 'lgb_model_with_constraints.txt', num_iteration=bst.best_iteration)\n",
    "    else:\n",
    "        bst.save_model(model_path / 'lgb_model_no_constraints.txt', num_iteration=bst.best_iteration)\n",
    "    print(\"Model training is done.\")\n",
    "\n",
    "    y_score = bst.predict(test[0], num_iteration=bst.best_iteration)\n",
    "    rho, p = spearmanr(y_score, test[1])\n",
    "    print(f\"\\nTesting the model on {TEST_YEAR} data:\")\n",
    "    print(f\"Spearman rank corr={rho:.2%} (p-value={p:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f9cfa32f-d96b-4fe6-930f-bda7ea3f06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'huber',\n",
    "        'boosting': 'gbdt',  # gbdt or dart\n",
    "        'drop_rate': 0.1,    # dropout rate in dart\n",
    "        'num_boost_round': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 25,\n",
    "        'num_threads': 4,\n",
    "        'bagging_fraction': 0.8, # fraction for subsampling data (<=1)\n",
    "        'bagging_freq': 1, # 0 means no bagging\n",
    "        'alpha': 0.9,   # huber loss parameter\n",
    "        'metric': ['mae', 'mse', 'huber'],\n",
    "        'min_data_in_leaf': 1,\n",
    "        'force_col_wise': True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0c351e13-8bbd-4ae3-8c0c-420c0d0fd1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 97925\n",
      "[LightGBM] [Info] Number of data points in the train set: 38822, number of used features: 385\n",
      "[LightGBM] [Info] Start training from score 0.005824\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 0.0419229\tvalid_0's l2: 0.00533583\tvalid_0's huber: 0.00266792\n",
      "Model training is done.\n",
      "\n",
      "Testing the model on 2016 data:\n",
      "Spearman rank corr=2.69% (p-value=0.002)\n"
     ]
    }
   ],
   "source": [
    "train_booster_and_test(train_val_test, params, constraint_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "554c29a4-aaee-43b5-82a6-2f591d445b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 391680\n",
      "[LightGBM] [Info] Number of data points in the train set: 9705, number of used features: 1536\n",
      "[LightGBM] [Info] Start training from score 0.005892\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's l1: 0.0398812\tvalid_0's l2: 0.00427291\tvalid_0's huber: 0.00213646\n",
      "Model training is done.\n",
      "\n",
      "Testing the model on 2016 data:\n",
      "Spearman rank corr=2.95% (p-value=0.088)\n"
     ]
    }
   ],
   "source": [
    "train_booster_and_test(train_val_test, params, constraint_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3d3bd-7f8d-4594-a5de-b88da7c825c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cbd381a-add3-4bd5-860b-83abbbd42f42",
   "metadata": {},
   "source": [
    "The above results suggest that **Information Coeffecient** is not highly significant when we train the booster on different sections (p-value=0.088). This is mainly due to smaller train set size (10k vs. 40k doc vectors) and more number of features (1536 vs. 385). More training data is needed to attain a clear conclusion on which training logic performs better.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2f81c-ec98-41c2-b386-b1f95b74e14f",
   "metadata": {},
   "source": [
    "## Train Gradient Boosting Model with SEC Document Embeddings from Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d72b77ba-42cc-4840-8183-6834bcda5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "# this model is fine tuned on nreimers/MiniLM-L6-H384-uncased pre-trained model\n",
    "model_ckpt = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
    "sbert_model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)\n",
    "\n",
    "MAX_LENGTH = tokenizer.max_len_single_sentence\n",
    "BATCH_SIZE = 16\n",
    "SECTIONS = ['section_1', 'section_1A', 'section_7', 'section_7A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9afaeafc-6954-421e-9a27-cd7664d3b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask, sample_map):\n",
    "    batch_result = []\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = tf.expand_dims(tf.cast(attention_mask, tf.float32), -1)\n",
    "    # Perform mean pooling over chunks of each sequence\n",
    "    for i in range(tf.reduce_max(sample_map) + 1):\n",
    "        s_idx = np.where(sample_map==i)[0][0]\n",
    "        e_idx = np.where(sample_map==i)[0][-1] + 1\n",
    "        sum_embeddings = tf.reduce_sum(token_embeddings[s_idx:e_idx] * input_mask_expanded[s_idx:e_idx], axis=[0, 1])\n",
    "        sum_mask = tf.clip_by_value(tf.reduce_sum(input_mask_expanded[s_idx:e_idx]), clip_value_min=1e-7, clip_value_max=np.inf)\n",
    "        batch_result += [sum_embeddings / sum_mask]\n",
    "        \n",
    "    return tf.stack(batch_result)\n",
    "\n",
    "def get_transformer_embeddings(data, col_name):\n",
    "    encoded_input = tokenizer(\n",
    "        data[col_name],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_overflowing_tokens=True,  # Large sequences are chunked into arrays of max_length\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    # Identifies what sequence chunk belongs to which sample\n",
    "    sample_map = encoded_input.pop(\"overflow_to_sample_mapping\")\n",
    "    attention_mask = encoded_input[\"attention_mask\"]\n",
    "    model_output = sbert_model(**encoded_input)\n",
    "    \n",
    "    return {\n",
    "        \"cik\": data[\"cik\"],\n",
    "        \"year\": data[\"year\"],\n",
    "        \"section\": data[\"section\"],\n",
    "        f\"{col_name}_embedding\": mean_pooling(model_output, attention_mask, sample_map),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "be19b2cf-b733-4e59-b6d1-4e5485944484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(data_path.as_posix())\n",
    "train = datasets.concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]], axis=0)\n",
    "# keep only those companies for which we have price data from 2013-2016\n",
    "train = train.filter(lambda examples: (examples['cik'], examples['year']) in CIK_YEAR, num_proc=NUM_PROC)\n",
    "\n",
    "test = train.filter(lambda row: int(row['year']) == TEST_YEAR)\n",
    "train = train.filter(lambda row: int(row['year']) in years[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6257dc4a-ab7c-4750-9d8a-54b712550646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of 750 companies to speedup embeddings generation on a small machine\n",
    "SUBSET_SIZE = 750\n",
    "subset = np.random.choice(np.array(list(set(train['cik']).intersection(set(test['cik'])))), size=SUBSET_SIZE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a2536315-2819-46e8-93e1-9170bdcb0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset_and_wrap(data, subset, col_names, new_name):\n",
    "    data = data.filter(lambda row: row['cik'] in subset)\n",
    "    \n",
    "    wrapped = []\n",
    "    for col in col_names:\n",
    "        temp = data.select_columns(['cik', 'year', col])\n",
    "        temp = temp.filter(lambda example: example[col] != '')\n",
    "        temp = temp.add_column(name='section',  column=[col.split('_')[-1]] * len(temp))\n",
    "        temp = temp.rename_column(col, new_name)\n",
    "        wrapped.append(temp)\n",
    "\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5f75da06-fac0-4182-9426-fef687c97aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25923ab8a8e48eeb1b86cf83c61a1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bc80f165b94d448c74849e3732bdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f7e999c7a740089f1448f2b55d62ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627068ab5b6644aeb126b44b176dbce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac501698d4dc46ccb3768e389e784b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3758484de18641fc92110a0e8cbe1475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b830232beb445ba15203f8ec73cd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bc1ee2f6514e378039cccd8aa4cd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583cb5ae6bc24a109192039cd045f376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e17a413e10d4ba49ba79dbb2485448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c92d573d604bca9e03355359ce9d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc563127ea14a11bae4b9f5a1f0c173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037c8b53a0e24e178b8697f0940ad72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424cfc3e93b14e2abcffb26431cd81b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50aaf7eef9f54de394d146f9ec7e808b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aaf08287744ef5997c6ef340fc6144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/730 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659caa6683934bc6a7637a2c9bbb96f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dafa36defc4e5ca8454e30e6c7efe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate all sections of raw SEC filings in one column to be fed to the transformer model\n",
    "wrapped = select_subset_and_wrap(train, subset, SECTIONS, new_name='raw_sections')\n",
    "train = datasets.concatenate_datasets(wrapped, axis=0)\n",
    "\n",
    "wrapped = select_subset_and_wrap(test, subset, SECTIONS, new_name='raw_sections')\n",
    "test = datasets.concatenate_datasets(wrapped, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "be4b3d19-b166-40f6-ba2d-2c4c9622f01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482e37a31aac418482b8371a36dff258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8049 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657885563a1f4383ac3d31ac6998291c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_embeddings = train.map(get_transformer_embeddings, batched=True, batch_size=BATCH_SIZE, remove_columns=test.column_names, fn_kwargs={'col_name': 'raw_sections'})\n",
    "test_embeddings = test.map(get_transformer_embeddings, batched=True, batch_size=BATCH_SIZE, remove_columns=test.column_names, fn_kwargs={'col_name': 'raw_sections'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0ebea571-d8ee-4391-b683-c04dfa43e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_transformer = pd.concat([\n",
    "    train_embeddings.to_pandas().set_index(['cik' , 'year'], drop=True).join(returns),\n",
    "    test_embeddings.to_pandas().set_index(['cik' , 'year'], drop=True).join(returns)\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "072d107e-8ada-4d31-a299-35875db2cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_transformer.to_hdf(sec_path / 'sec_returns.h5', 'data/doc_vecs_return_transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a50b17-9b8d-4ad2-b957-5f40c7ced128",
   "metadata": {},
   "source": [
    "### Split data and train a LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "57516fd4-6aec-4164-b59b-774f26fafdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_transformer = pd.read_hdf(sec_path / 'sec_returns.h5', 'data/doc_vecs_return_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e810fbdd-4b63-454b-839a-fb7b17e3f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_booster_and_test_for_transformer(data, params):\n",
    "    (train, valid), test, constraints = prepare_data_for_training(data, [\"raw_sections_embedding\"], constraint_features=False)\n",
    "\n",
    "    params['interaction_constraints'] = constraints\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params=params,\n",
    "        train_set=train,\n",
    "        valid_sets=valid,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
    "    )\n",
    "    \n",
    "    bst.save_model(model_path / 'lgb_model_for_transformer.txt', num_iteration=bst.best_iteration)\n",
    "    print(\"Model training is done.\")\n",
    "\n",
    "    y_score = bst.predict(test[0], num_iteration=bst.best_iteration)\n",
    "    rho, p = spearmanr(y_score, test[1])\n",
    "    print(f\"\\nTesting the model on {TEST_YEAR} data:\")\n",
    "    print(f\"Spearman rank corr={rho:.2%} (p-value={p:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "77e745c3-267f-4505-bb8b-c453dc86aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'huber',\n",
    "        'boosting': 'gbdt',  # or dart\n",
    "        'drop_rate': 0.1,    # dropout rate in dart\n",
    "        'num_boost_round': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 25,\n",
    "        'num_threads': 4,\n",
    "        'bagging_fraction': 0.8, # fraction for subsampling data (<=1)\n",
    "        'bagging_freq': 1, # 0 means no bagging\n",
    "        'alpha': 0.9,   # huber loss parameter\n",
    "        'metric': ['mae', 'mse', 'huber'],\n",
    "        'min_data_in_leaf': 1,\n",
    "        'force_col_wise': True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6f7e1821-62d7-4d2f-9cd7-4e071b8f5d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 97925\n",
      "[LightGBM] [Info] Number of data points in the train set: 7244, number of used features: 385\n",
      "[LightGBM] [Info] Start training from score 0.006355\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0400413\tvalid_0's l2: 0.00481805\tvalid_0's huber: 0.00240902\n",
      "Model training is done.\n",
      "\n",
      "Testing the model on 2016 data:\n",
      "Spearman rank corr=-2.34% (p-value=0.213)\n"
     ]
    }
   ],
   "source": [
    "train_booster_and_test_for_transformer(train_val_test_transformer, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e75b12-806f-415b-af96-d0a519132447",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b23f588-41a0-4731-ab2f-b33cab01a5c6",
   "metadata": {},
   "source": [
    "High p-value for information coefficient here again suggests that the train set size is not large enough to draw any conclusion. This is reasonable as we have selected a subset of 750 companies to generate the embedding vectors due to demanding computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
